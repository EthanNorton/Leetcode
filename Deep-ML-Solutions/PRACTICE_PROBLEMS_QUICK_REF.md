# CMU Practice Problems - Quick Reference

## üéØ Problem Categories by Course

### **10-701/715: Introduction to ML**
- ‚úÖ Matrix operations (multiplication, reshaping)
- ‚úÖ Evaluation metrics (accuracy, precision, recall, F1, ROC-AUC)
- ‚úÖ Linear regression (normal equation, gradient descent)
- ‚úÖ Bias-variance tradeoff
- ‚úÖ Overfitting and regularization

### **10-617/707: Deep Learning**
- ‚úÖ Activation functions (ReLU, sigmoid, softmax)
- ‚úÖ Neural network forward/backward pass
- ‚úÖ Convolutional operations
- ‚úÖ Backpropagation calculations

### **10-725: Optimization**
- ‚úÖ Gradient descent variants (batch, mini-batch, stochastic)
- ‚úÖ Momentum
- ‚úÖ Learning rate scheduling
- ‚úÖ Convergence analysis

### **10-718: ML in Practice**
- ‚úÖ Feature scaling (min-max, standardization)
- ‚úÖ Missing data handling
- ‚úÖ Categorical encoding (one-hot, label)
- ‚úÖ Data preprocessing pipelines

### **36-700/705: Probability & Statistics**
- ‚úÖ Bayes' theorem
- ‚úÖ Maximum likelihood estimation
- ‚úÖ Probability distributions
- ‚úÖ Statistical inference

---

## üìä Difficulty Distribution

| Difficulty | Count | Focus Areas |
|-----------|-------|-------------|
| ‚≠ê Beginner | 8 | Fundamentals, basic implementations |
| ‚≠ê‚≠ê Intermediate | 12 | Standard algorithms, applications |
| ‚≠ê‚≠ê‚≠ê Advanced | 5 | Theory, derivations, optimizations |

---

## üéì Exam Preparation Strategy

### **Week 1-2: Fundamentals**
- Master matrix operations
- Understand evaluation metrics
- Practice basic implementations

### **Week 3-4: Core Algorithms**
- Linear regression (both methods)
- Neural network forward pass
- Activation functions

### **Week 5-6: Advanced Topics**
- Backpropagation
- Optimization variants
- Regularization

### **Week 7: Review & Practice**
- Work through all problems
- Time yourself on exam-style questions
- Review theory explanations

---

## üîë Key Formulas to Memorize

### **Linear Regression**
- Normal Equation: `Œ∏ = (X^T X)^(-1) X^T y`
- Gradient: `‚àáJ = (1/m) X^T (XŒ∏ - y)`
- Update: `Œ∏ = Œ∏ - Œ±‚àáJ`

### **Activation Functions**
- ReLU: `f(x) = max(0, x)`
- Sigmoid: `f(x) = 1/(1 + e^(-x))`
- Softmax: `f(x_i) = e^(x_i) / Œ£e^(x_j)`

### **Evaluation Metrics**
- Accuracy: `(TP + TN) / (TP + TN + FP + FN)`
- Precision: `TP / (TP + FP)`
- Recall: `TP / (TP + FN)`
- F1: `2 √ó (Precision √ó Recall) / (Precision + Recall)`

### **Probability**
- Bayes' Theorem: `P(A|B) = P(B|A) √ó P(A) / P(B)`
- MLE for Normal Mean: `Œº = (1/n) Œ£x_i`

---

## üí° Common Exam Question Types

1. **Implementation:** Code an algorithm from scratch
2. **Theory:** Explain why/how something works
3. **Calculation:** Manual computation (e.g., gradient, probability)
4. **Comparison:** Compare two methods/algorithms
5. **Debugging:** Find and fix bugs in code
6. **Application:** Choose appropriate method for a scenario

---

## üìö Study Resources

- **Full Problems:** See `CMU_PRACTICE_PROBLEMS.md`
- **CMU Course Sites:** Check for past exams
- **Textbooks:** Bishop, Goodfellow, Boyd
- **Practice:** Implement all problems yourself

---

**Start with ‚≠ê problems, then move to ‚≠ê‚≠ê and ‚≠ê‚≠ê‚≠ê!**

