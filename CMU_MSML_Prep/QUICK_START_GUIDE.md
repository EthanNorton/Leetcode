# Quick Start Guide - Example Problems

**Ready to practice? Here are guided walkthroughs for more problems!**

---

## ðŸŽ¯ Problems with Complete Walkthroughs

### âœ… Already Covered (Detailed):
1. **ReLU** - Complete walkthrough with sparsity explanation
2. **Gradient Descent** - Conceptual deep dive + examples
3. **Matrix times Vector** - Step-by-step with real examples
4. **One-Hot Encoding** - Complete explanation
5. **Softmax** - Comprehensive walkthrough
6. **Feature Scaling** - High-level overview
7. **Evaluation Metrics** - CMU-focused overview

### ðŸ“š New Problems Available:

#### 1. **Sigmoid Activation Function**
**Location:** `Deep-ML-Solutions/Sigmoid Activation Function Understanding/`
**Guide:** `CMU_MSML_Prep/01_Foundations/Activation_Functions/SIGMOID_GUIDE.md`

**What it does:**
- Squashes inputs to 0-1 range
- Used for binary classification
- Outputs probabilities

**Key concepts:**
- S-shaped curve
- Vanishing gradient problem
- When to use vs ReLU

**CMU Connection:** 10-617/707 (Deep Learning), 10-701/715 (Logistic Regression)

---

#### 2. **Reshape Matrix**
**Location:** `Deep-ML-Solutions/Reshape Matrix/`
**Guide:** `CMU_MSML_Prep/01_Foundations/Matrix_Operations/RESHAPE_GUIDE.md`

**What it does:**
- Changes matrix dimensions
- Keeps all same elements
- Reorganizes data

**Key concepts:**
- Row-major order
- Total elements must match
- CNN to FC layer connection

**CMU Connection:** 10-617/707 (Deep Learning), 10-725 (Matrix Operations)

---

#### 3. **Linear Regression (Normal Equation)**
**Location:** `Deep-ML-Solutions/Linear Regression Using Normal Equation/`

**What it does:**
- Analytical solution (no iteration!)
- Uses matrix operations
- Alternative to gradient descent

**Key concepts:**
- Pseudoinverse
- When to use vs gradient descent
- Computational trade-offs

**CMU Connection:** 10-725 (Optimization), 10-701/715 (ML)

---

## ðŸš€ How to Practice

### Step 1: Read the Problem
- Understand what it does
- Read the CMU connection
- See why it matters

### Step 2: Try to Implement
- Code from scratch
- Don't look at solution first
- Test with examples

### Step 3: Compare & Learn
- Check provided solution
- Understand differences
- Learn from mistakes

### Step 4: Apply to Real Data
- Use on actual datasets
- See how it works in practice
- Connect to CMU courses

---

## ðŸ“– Study Order Recommendation

### Week 1: Activation Functions
1. ReLU (already done!)
2. Sigmoid (new!)
3. Softmax (already done!)
4. Compare all three

### Week 2: Matrix Operations
1. Matrix times Vector (already done!)
2. Reshape Matrix (new!)
3. Practice combining them

### Week 3: Optimization
1. Gradient Descent (already done!)
2. Normal Equation (new!)
3. Compare approaches

---

## ðŸ’¡ Tips for Success

1. **Start simple** - Understand the concept first
2. **Code it** - Don't just read, implement!
3. **Test thoroughly** - Use provided examples
4. **Connect to CMU** - See how it fits courses
5. **Think research** - How does this relate?

---

## ðŸŽ“ Next Steps

1. Work through Sigmoid (binary classification)
2. Master Reshape Matrix (neural network prep)
3. Try Normal Equation (analytical optimization)
4. Connect all concepts together

**All problems are in your CMU_MSML_Prep folder!**

