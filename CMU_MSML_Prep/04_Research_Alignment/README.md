# Research Alignment - CMU MSML

**Purpose:** Connect problems to CMU research interests, especially Dr. Shah's work.

---

## Dr. Shah's Research Areas

### 1. Evaluation Science
**Key Papers:**
- "The More You Automate, The Less You See"
- Evaluation pitfalls in ML
- Benchmark evaluation

**Relevant Problems:**
- **Evaluation Metrics** (Calculate Accuracy Score)
  - Understanding evaluation pitfalls
  - When metrics are misleading
  - Evaluation bias

**Connection:**
- Your understanding of accuracy, precision, recall
- Evaluation challenges (data leakage, distribution shift)
- Directly connects to evaluation science research

---

### 2. Annotation Bias
**Key Concepts:**
- Reviewer assignment
- Annotation quality
- Bias in labeled data

**Relevant Problems:**
- **One-Hot Encoding**
  - How we represent categorical data
  - Preprocessing choices affect models
  - Feature engineering

- **Feature Scaling**
  - How preprocessing affects evaluation
  - Data representation matters

**Connection:**
- Preprocessing decisions can introduce bias
- Understanding data pipeline connects to annotation research

---

### 3. Reproducibility
**Key Concepts:**
- Reproducible ML pipelines
- Evaluation reproducibility
- Seed stability

**Relevant Problems:**
- **Gradient Descent**
  - Random initialization
  - Seed stability
  - Reproducibility in optimization

- **Evaluation Metrics**
  - Reproducible evaluation
  - Train/test splits
  - Cross-validation

**Connection:**
- Understanding reproducibility challenges
- Can contribute to reproducibility research

---

## Research Project Ideas

### Project 1: Seed Stability Analysis
**Problems Used:**
- Gradient Descent
- Evaluation Metrics
- Feature Scaling

**Research Question:**
- How stable are ML models across different random seeds?
- Does preprocessing affect seed stability?

**CMU Alignment:**
- 10-718 (Reproducibility)
- 10-725 (Optimization)
- Evaluation science research

---

### Project 2: Evaluation Bias Audit
**Problems Used:**
- Evaluation Metrics
- One-Hot Encoding
- Feature Scaling

**Research Question:**
- How do evaluation choices affect reported performance?
- Can we detect evaluation bias?

**CMU Alignment:**
- 10-718 (Evaluation Science)
- Dr. Shah's research area
- Direct research contribution opportunity

---

### Project 3: Preprocessing Impact Study
**Problems Used:**
- Feature Scaling
- One-Hot Encoding
- Evaluation Metrics

**Research Question:**
- How do preprocessing choices affect model performance?
- Are some preprocessing methods more robust?

**CMU Alignment:**
- 10-718 (ML in Practice)
- Evaluation science
- Practical ML research

---

## Connecting Problems to Research

### Evaluation Metrics → Evaluation Science
- Understanding when metrics fail
- Evaluation pitfalls
- Research on evaluation methodology

### Preprocessing → Annotation Bias
- How data representation affects models
- Bias in feature engineering
- Data pipeline research

### Optimization → Reproducibility
- Seed stability
- Reproducible optimization
- Research on ML reproducibility

---

## Next Steps for Research

1. **Master the fundamentals** (all problems in 01_Foundations/)
2. **Understand evaluation deeply** (evaluation science connection)
3. **Read Dr. Shah's papers** (evaluation, annotation bias)
4. **Identify research questions** (connect problems to research)
5. **Design projects** (use problems as building blocks)

---

## Research Resources

### Papers to Read:
- "The More You Automate, The Less You See" (Dr. Shah)
- Evaluation science papers
- Annotation bias research
- Reproducibility in ML

### CMU Resources:
- Dr. Shah's lab website
- CMU ML research groups
- Evaluation science seminars
- Reproducibility workshops

---

## Bottom Line

**These problems aren't just exercises - they're building blocks for research!**

Understanding:
- Evaluation metrics → Evaluation science research
- Preprocessing → Annotation bias research
- Optimization → Reproducibility research

**Master the fundamentals, then connect to research!**

